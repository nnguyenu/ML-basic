{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlmgPCesHxDI"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE78zQwDHxDL"
      },
      "outputs": [],
      "source": [
        "NAME = \"\"\n",
        "COLLABORATORS = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy-tnrTGHxDL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "496d8b003219f6d77db221f1ebe1148d",
          "grade": false,
          "grade_id": "cell-a25c0c792fa938b1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "S_1ldr36HxDM"
      },
      "source": [
        "# CSE 204 Lab 11: Unsupervised Learning - Clustering\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/adimajo/polytechnique-cse204-2019-releases/master/logo.jpg\" style=\"float: left; width: 15%\" />\n",
        "\n",
        "[CSE204-2019](https://moodle.polytechnique.fr/course/view.php?id=7862) Lab session #11\n",
        "\n",
        "J.B. Scoggins, Jesse Read, Adrien Ehrhardt, Jérémie Decock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ddc1d13834647030b3a778387d3a14b6",
          "grade": false,
          "grade_id": "cell-21cb208f70943b4f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ACmq8IkRHxDM"
      },
      "source": [
        "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adimajo/polytechnique-cse204-2019-releases/blob/master/lab_session_11/lab_session_11.ipynb)\n",
        "\n",
        "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/adimajo/polytechnique-cse204-2019-releases/master?filepath=lab_session_11%2Flab_session_11.ipynb)\n",
        "\n",
        "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/adimajo/polytechnique-cse204-2019-releases/raw/master/lab_session_11/lab_session_11.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1417825c3dfd85900e0b6c8a03d9051d",
          "grade": false,
          "grade_id": "cell-9ed05ba8846d12be",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ZEhD-KdRHxDM"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this lab, you will implement two unsupervised learning algorithms cluster data points based on similarity criteria: k-means, and spectral k-means.  While libraries such as scikit-learn provide facilities that implement these algorithms, they are simple enough for you to implement with numpy alone.  Before beginning, import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b68a305b2888ea9c1418733005150f77",
          "grade": false,
          "grade_id": "cell-49e97acfb570741d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "SsmhwVmkHxDN"
      },
      "outputs": [],
      "source": [
        "colab_requirements = [\n",
        "    \"matplotlib>=3.1.2\",\n",
        "    \"numpy>=1.18.1\",\n",
        "    \"nose>=1.3.7\",\n",
        "]\n",
        "import sys, subprocess\n",
        "def run_subprocess_command(cmd):\n",
        "    # run the command\n",
        "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
        "    # print the output\n",
        "    for line in process.stdout:\n",
        "        print(line.decode().strip())\n",
        "        \n",
        "if \"google.colab\" in sys.modules:\n",
        "    for i in colab_requirements:\n",
        "        run_subprocess_command(\"pip install \" + i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1c96b2467626b794e60bae14ac63cb56",
          "grade": false,
          "grade_id": "cell-ad810c8a728ffb57",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "LNDT8NquHxDN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randrange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "37a29e94c1f65a325b14f968a23af46f",
          "grade": false,
          "grade_id": "cell-410c51ab5d8c4bcf",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ABdyfsOsHxDN"
      },
      "source": [
        "### Datasets\n",
        "\n",
        "Throughout this lab, you will use 3 simple datasets to test your algorithms.  Run the code below to visualize each dataset. As you can see, the first dataset consists of 4 gaussian-distributed clusters of points with equal variance.  The second represents two clusters, one stretched vertically, and one horizontally.  Finally, the last dataset represents 3 clusters distributed in rings.  For convenience, the three datasets are placed in a list called `datasets`.  In the rest of the lab, you will be asked to implement 2 clustering algorithms and run on them on these datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4710f563443f1d55faeed3227a2cff16",
          "grade": false,
          "grade_id": "cell-3a20658d8a1c2631",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "kkPSr39bHxDO"
      },
      "outputs": [],
      "source": [
        "# Create a data set\n",
        "N = 120\n",
        "\n",
        "data1 = np.random.normal((0,0), (0.5,0.5) ,size=(N,2))\n",
        "data1 = np.append(data1, np.random.normal((5,0), (0.5,0.5), size=(N,2)), axis=0)\n",
        "data1 = np.append(data1, np.random.normal((0,5), (0.5,0.5), size=(N,2)), axis=0)\n",
        "data1 = np.append(data1, np.random.normal((5,5), (0.5,0.5), size=(N,2)), axis=0)\n",
        "\n",
        "data2 = np.random.normal((2,5), (0.25, 1), size=(N,2))\n",
        "data2 = np.append(data2, np.random.normal((5,5), (1, 0.25), size=(N,2)), axis=0)\n",
        "\n",
        "radii = np.random.normal(0,0.5,size=(N,1))\n",
        "radii = np.append(radii, np.random.normal(4,0.5,size=(2*N,1)), axis=0)\n",
        "radii = np.append(radii, np.random.normal(8,0.5,size=(3*N,1)), axis=0)\n",
        "angles = np.random.uniform(size=(6*N,1))*2.0*np.pi\n",
        "data3 = np.hstack([radii*np.cos(angles), radii*np.sin(angles)])\n",
        "\n",
        "datasets = [data1, data2, data3]\n",
        "\n",
        "fig, axes = plt.subplots(1,len(datasets), figsize=(10,3))\n",
        "for i,data in enumerate(datasets):\n",
        "    axes[i].scatter(data[:,0], data[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "74e5a374a4ccaa9f10b37b82e2060577",
          "grade": false,
          "grade_id": "cell-8fb6fd761d643f5c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "vjEPupiJHxDO"
      },
      "source": [
        "## Part 1: The k-Means Algorithm\n",
        "\n",
        "k-means is one of the simplest unsupervised learning algorithms that solves the well known clustering problem. The algorithm defines an iterative process, where the following two steps take part at each iteration:\n",
        "1. take each instance belonging to the dataset and assign it to the nearest centroid, and\n",
        "2. re-calculate the centroids of each of the k clusters. \n",
        "Thus, the k centroids change their location step by step until no more changes are done.\n",
        "\n",
        "More formally, suppose that we are given a dataset $X = \\{x_1, x_2, \\dots , x_n\\}$, where each $x_i \\in \\mathbb{R}^d$. The goal of the k-means algorithm is to group the data into $k$ cohesive clusters, where $k$ is an input parameter of the algorithm. **Your task is to implement this algorithm**. Algorithm 1 gives the pseudocode.\n",
        "\n",
        "___\n",
        "### Algorithm 1: k-means\n",
        "\n",
        "**Input**: The dataset $\\mathbf{X} = \\Big( \\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_n \\Big)$ (where each $\\mathbf{x}_i \\in \\mathbb{R}^d$) and the parameter $k$ (the number of clusters)<br>\n",
        "**Output**: Clusters assignments (the cluster assigned to each element of $\\mathbf{X}$)\n",
        "\n",
        "1. Initialize cluster centroids ${\\boldsymbol\\mu}_1, \\boldsymbol{\\mu}_2, \\ldots, \\boldsymbol{\\mu}_k$ by choosing $k$ instances of $\\mathbf{X}$ randomly\n",
        "\n",
        "**Repeat:**\n",
        "2. Assign each instance $\\mathbf{x}_i \\in \\mathbf{X}$ to the closest centroid, i.e., $c_i = \\text{argmin}_j \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_j\\|$\n",
        "3. Re-compute the centroids $\\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\ldots, \\boldsymbol{\\mu}_k$ of each cluster based on $\\boldsymbol{\\mu}_j = \\Big(\\sum_{\\mathbf{x} \\in \\mathbf{C}_j} \\mathbf{x}\\Big)/|\\mathbf{C}_j|$, where $\\mathbf{C}_j = \\{\\mathbf{x}_i  c_i = j\\}$ the $j$-th cluster with $j=1, \\ldots, k$ and $|\\mathbf{C}_j|$ the size of the $j$-th cluster\n",
        "\n",
        "**until** Centroids do not change (convergence)\n",
        "___\n",
        "\n",
        "In the algorithm above, $k$ is a parameter of the algorithm and corresponds to the number of clusters we want to find; the cluster centroids $\\mu_j$ represent our current guesses for the positions of the centers of the clusters. To initialize the cluster centroids (in step 1 of the algorithm), we could choose $k$ training examples randomly, and set the cluster centroids to be equal to the values of these $k$ examples. Of course, other initialization methods are also possible, such as the [kmeans++ technique](https://en.wikipedia.org/wiki/K-means%2B%2B). To find the closest centroid, a distance (or similarity) function should be defined, and typically the Euclidean distance is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c5e1ee8b93ee0da6e45668ea7ee1308f",
          "grade": false,
          "grade_id": "cell-69effc57b6477d10",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "xoIN8DjuHxDP"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(X1, X2):\n",
        "    \"\"\"\n",
        "    Distance Function\n",
        "    -----------------\n",
        "    Computes the Euclidean distance between two arrays of points.\n",
        "    \n",
        "    Return\n",
        "    ------\n",
        "    A 2D n by m array where entry [i,k] is the distance \n",
        "    from the i-th point in X1 to the k-th point in X2.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "59a5fbf32b82a181b64c53195058a0d4",
          "grade": false,
          "grade_id": "cell-3bb79d305ce4e0c7",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "bymD7AkMHxDP"
      },
      "outputs": [],
      "source": [
        "def k_means(X, k):\n",
        "    \"\"\"\n",
        "    k Means\n",
        "    --------\n",
        "    \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    \n",
        "    X : an n-by-d matrix of inputs\n",
        "    k : the number of clusters to find\n",
        "    \n",
        "    \n",
        "    Algorithm:\n",
        "    ----------\n",
        "    \n",
        "    0. Implement the euclidean distance function\n",
        "    1. Initialize (choose) the centroids\n",
        "    2. Implement a `while` loop such that, while centroids have not changed since the last iteration:\n",
        "        - compute the distances of all points to each centroid\n",
        "        - label each point (associate it with) the nearest centroid\n",
        "        - recompute the centroids (i.e., average of points belonging to each centroid)\n",
        "    \n",
        "    Return\n",
        "    ------\n",
        "    \n",
        "    z : a 1D vector of labels of length n (e.g. z[i] = c_j means \"x_i is belongs to cluster c_j\")\n",
        "    iters : the number of iterations carried out until convergence\n",
        "    \"\"\"\n",
        "    # Initialize (choose) the centroids\n",
        "    # Iterate until convergence\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    return z, iters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2937c6f993ff0c5a85a459d368efc1af",
          "grade": false,
          "grade_id": "cell-a62b149919aabbb4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "AH79nt2-HxDP"
      },
      "source": [
        "To test your implementation, run the following code which will plot the 3 datasets, trying differente values of $k$. It will display the number of iterations until convergence (along with $k$ in the title)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b9fe22a384949a8a0239fb1bd3658011",
          "grade": false,
          "grade_id": "cell-a4bb80c820df2c24",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "3mYD6rNeHxDP"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3,len(datasets),figsize=(10,10))\n",
        "for i,k in enumerate([2,3,4]):\n",
        "    for j,data in enumerate(datasets):\n",
        "        labels, iters = k_means(data, k)\n",
        "        axes[i,j].scatter(data[:,0], data[:,1], c = labels, cmap='rainbow')\n",
        "        axes[i,j].set_title('$k=%d$, iter$=%d$' % (k,iters))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "7923fe41153a1dbb6dbccdb3849c7eb6",
          "grade": false,
          "grade_id": "cell-1f9ecd36547f81b4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "aPrM6tfUHxDP"
      },
      "source": [
        "Based on its notion of similarity, the problem of $k$-means clustering can be reduced to the problem of finding appropriate centroids. This, in turn, can be expressed as the task of minimizing the following objective function:\n",
        "$$\n",
        "     E(k) = \\sum_{j=1}^{k} \\sum_{\\mathbf{x}_i \\in \\mathbf{C}_j}\\| \\mathbf{x}_i - \\boldsymbol{\\mu}_j \\|^2.\n",
        "$$\n",
        "\n",
        "Thus, minimizing the above function is to determine suitable centroids $\\mathbf{\\mu}_j$ such that, if the data is partitioned into corresponding clusters $\\mathbf{C}_j$, distances between data points and their closest cluster centroid become as small as possible.\n",
        "\n",
        "The convergence of the $k$-means algorithm is highly dependent on the initialization of the centroids. It may converge to a local minimum of the objective function above. One way to overcome this problem is by executing the algorithm several times, with different initializations of the centroids. \n",
        "\n",
        "Another issue is how to determine the number of clusters ($k$) of the dataset. Intuitively, increasing $k$ without penalty, will always reduce the amount of error in the resulting clustering, to the extreme case of zero error if each data point is considered its own cluster (i.e., when $k=n$). One such method is known as the *elbow rule*. The idea is to examine  and compare the error given above for a number of cluster solutions.  In general, as the number of clusters increases, the *sum of squared errors* (SSE) should decrease because clusters are, by definition, smaller. A plot of the SSE against a series of sequential cluster levels (i.e., different values) can be helpful here. That is, an appropriate cluster solution could be defined as the one where the reduction in SSE slows dramatically. This produces an \"elbow\" in the plot of SSE against the different values of $k$. \n",
        "\n",
        "Bonus Task: Implement the elbow rule to find an appropriate value for $k$, as follows:\n",
        "\n",
        "1. Run k-means clustering for values of $k=1,\\ldots,10$. \n",
        "2. For each $k$, calculate the total intra-cluster error ($E(k)$, given above)\n",
        "3. Plot the curve of $E(k)$ vs $k$.\n",
        "4. Try to identify the location of a bend (elbow) in the plot -- this is generally considered as an indicator of the appropriate number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "67dbbe0fa7bb488d5a8ce97a44de25bb",
          "grade": false,
          "grade_id": "cell-c48684499f1ff542",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "mhFRpkGPHxDP"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4ae9a26917bd59859904f17659308668",
          "grade": false,
          "grade_id": "cell-a31d9197f0a12b42",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "18WIV0JJHxDQ"
      },
      "source": [
        "## Part II: Spectral Clustering\n",
        "\n",
        "Spectral clustering techniques make use of the *spectrum* (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.\n",
        "\n",
        "Given a set of data points $\\mathbf{x}_1, \\ldots ,\\mathbf{x}_n, \\forall \\mathbf{x}_i \\in \\mathbb{R}^d$ and some notion of similarity $s_{ij}$ between all pairs of data points $\\mathbf{x}_i$ and $\\mathbf{x}_j$, the intuitive goal of clustering is to divide the data points into several groups such that points in the same group are similar and points in different groups are dissimilar to each other. If we do not have more information than similarities between data points, a nice way of representing the data is in form of the similarity graph $G = (V,E)$. Each vertex $v_i$ in this graph represents a data point\n",
        "$\\mathbf{x}_i$. Two vertices are connected if the similarity $s_{ij}$ between the corresponding data points $\\mathbf{x}_i$ and $\\mathbf{x}_j$ is positive or larger than a certain threshold, and the edge is weighted by $s_{ij}$. The problem of clustering\n",
        "can now be reformulated using the similarity graph: we want to find a partition of the graph such that the edges between different groups have very low weights (which means that points in different clusters are dissimilar from each other) and the edges within a group have high weights (which means that points within the same cluster are similar to each other).\n",
        "\n",
        "### Creating the similarity graph\n",
        "\n",
        "There are several popular constructions to transform a given set $\\mathbf{x}_1, \\ldots , \\mathbf{x}_m, \\forall \\mathbf{x}_i \\in \\mathbb{R}^n$ of data points with pairwise\n",
        "similarities $s_{ij}$ or pairwise distances $d_{ij}$ into a graph. When constructing similarity graphs the goal is to model the local neighborhood relationships between the data points. We will use the approach of the $k$-Nearest Neighbors graph. Here the goal is to connect vertex $\\mathbf{x}_i$ with vertex $\\mathbf{x}_j$ if $\\mathbf{x}_j$ is among the $k$-nearest neighbors of $\\mathbf{x}_i$. This definition leads to a directed graph, as the neighborhood relationship is not symmetric. The most common way to deal with this, is to simply ignore the directions of the edges; that is, we connect $\\mathbf{x}_i$ and $\\mathbf{x}_j$ with an undirected edge if $\\mathbf{x}_i$ is among the $k$-nearest neighbors of $\\mathbf{x}_j$ or if $\\mathbf{x}_j$ is among the $k$-nearest neighbors of $\\mathbf{x}_i$. The resulting graph is what is usually called the $k$-nearest neighbors graph.\n",
        "\n",
        "### The algorithm\n",
        "\n",
        "The pseudocode of the spectral clustering algorithm is given as follows. In spectral clustering, the data is projected into a lower-dimensional space (the spectral/eigenvector domain) where they are easily separable, say using $k$-means. \n",
        "\n",
        "Given dataset $\\mathbf{X}=\\{ \\mathbf{x_1}, \\mathbf{x_2}, \\ldots, \\mathbf{x_m}\\}$, where  each $\\mathbf{x}_i \\in \\mathbb{R}^n$ and parameter $k$:\n",
        "\n",
        "1. Construct the similarity graph $G$ as described above. Let \n",
        "    * $\\mathbf{W}$ be the adjacency matrix of this graph.\n",
        "    * $\\mathbf{D}$ be the diagonal degree matrix of graph $G$, ie $\\mathbf{D}_{ii} = \\sum_j \\mathbf{W}_{ij}$ and $\\mathbf{D}_{ij} = 0$ for $i \\neq j$.\n",
        "2. Compute the Laplacian matrix $\\mathbf{L} = \\mathbf{D} - \\mathbf{W}$. \n",
        "3. Apply eigenvalue decomposition to the Laplacian matrix $\\mathbf{L}$ \n",
        "4. Select the eigenvectors that correspond to $k$ smallest eigenvalues. Let $\\mathbf{U}$ contain these corresponding eigenvectors in its columns.\n",
        "5. Apply $k$-means to $\\mathbf{U}$ (as if the rows were instances), thus finding clusters $\\mathbf{C}_1, \\mathbf{C}_2, \\ldots, \\mathbf{C}_k$.\n",
        "\n",
        "**Your Task** Implement the algorithm. *Hint*: Use the distance function and the $k$-means implementation you wrote in Task 1.\n",
        "\n",
        "*Python hints:* to find the `k` nearest neighbors of $x_i$, you can use the function `np.argsort` with the array that stores `d(x_i, x_j)`. You can use `eigenvalues, eigenvectors = np.linalg.eig(L)` to obtain the eigenvalues and eigenvectors of a (square) matrix `L`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "391dd332f94c91770420f3f1622c5b3a",
          "grade": false,
          "grade_id": "cell-0178318004c9d59a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "MkiwA3IDHxDQ"
      },
      "outputs": [],
      "source": [
        "def spectral_k_means(X, k, k_nn):\n",
        "    '''\n",
        "        Spectral Clustering\n",
        "        -------------------\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        \n",
        "        X : the data\n",
        "        k : the number of clusters to find\n",
        "        k_nn : the number of k nearest-neighbours to consider in the graph construction\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        \n",
        "        The same numbers as your k-means method above\n",
        "    '''\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    return k_means(U, k)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "61b86531d3f7716b5fc0dd1c1c584eac",
          "grade": false,
          "grade_id": "cell-6ab3ebdcd041c832",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "3i--nbBUHxDQ"
      },
      "source": [
        "To test your implementation, run the following code which will plot the 3 datasets, trying different values of $k$. It will display the number of iterations (done by $k$-means) until convergence (along with $k$ in the title)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7bba670679de677bb8e4befe0f16200d",
          "grade": false,
          "grade_id": "cell-75c684606e463cf7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "3yyhQScFHxDQ"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, len(datasets), figsize=(10, 10))\n",
        "for i, k in enumerate([2, 3, 4]):\n",
        "    for j, data in enumerate(datasets):\n",
        "        labels, iters = spectral_k_means(data, k, 10)\n",
        "        axes[i,j].scatter(data[:,0], data[:,1], c=labels, cmap='rainbow')\n",
        "        axes[i,j].set_title('$k=%d$, iter$=%d$' % (k, iters))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e4d274a4ebc1bf0eb5e934f59ec249ce",
          "grade": false,
          "grade_id": "cell-16c31384dd31c8fe",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "v861B8DKHxDQ"
      },
      "source": [
        "## Task 3: Gaussian Mixture Model (Bonus Task)\n",
        "\n",
        "Suppose you observe and measure a number of beetles, recording their length in centimeters -- given below in the array `x`. You are curious to investigate if you have recorded more than one species. In fact you believe you are studying two distinct species. You decide to use Gaussian Mixture Models on the data to elaborate on this hypothesis and gain further insight into the beetle population. \n",
        "\n",
        "**Task: Implement Gaussian Mixture Models**\n",
        "\n",
        "Note: Plotting code is provided. \n",
        "Hint: See the lecture slides regarding pseudocode and outline. Note that you may obtain slightly different results each time you run the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e85145c9baa43d4b75de5fb1c1708254",
          "grade": false,
          "grade_id": "cell-3dee5fa76d753073",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "W6vwdTRnHxDQ"
      },
      "outputs": [],
      "source": [
        "# beetle lengths\n",
        "x = np.array([1.57, 1.16, 1.30, 0.26, 0.20, 0.43, 0.48,  0.34,  0.44,  0.61, 1.80, 1.40,  1.10,  1.25, 1.69], dtype=float)\n",
        "# no. of beetles\n",
        "N = len(x)\n",
        "# no. species\n",
        "K = 2\n",
        "\n",
        "# responsibilities (beetle i pertains r[i,k]-much to species k)\n",
        "r = np.random.random((N,K))\n",
        "# normalized\n",
        "r[:,1] = np.ones(N) - r[:,0]\n",
        "\n",
        "## evaluate (Gaussian) pdf 'g' at point 'x' under mean m, sd s\n",
        "def g(x, m = 0.0, s = 1.0):\n",
        "    return np.exp(-(x - m)**2/(2*s))/np.sqrt(2*s*np.pi)\n",
        "\n",
        "# Initialize parameters to random values\n",
        "m = np.array([0.0, 1.0])\n",
        "s = np.array([1.0] * 2)\n",
        "\n",
        "# Initialize weights (prior probabilities) aka pi\n",
        "w = np.ones(K)/N\n",
        "\n",
        "# EM ALGORITHM\n",
        "T = 100 # max iterations\n",
        "for t in range(0, T):\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        \n",
        "fig = plt.figure()\n",
        "xx = np.linspace(0, 2, num=100)      # points to plot\n",
        "plt.plot(xx,g(xx,m[0],s[0]),'r-',label=\"$g_1$\")\n",
        "plt.plot(xx,g(xx,m[1],s[1]),'b-',label=\"$g_2$\")\n",
        "plt.plot(xx,w[0]*g(xx, m[0], s[0]) + w[1] * g(xx, m[1], s[1]), 'm:', label=\"mixture\")\n",
        "c = np.sum(m)/2.\n",
        "y = (x < c)*1 \n",
        "plt.legend()\n",
        "plt.scatter(x,np.zeros(N), c=y, label=\"beetles\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (cse204)",
      "language": "python",
      "name": "cse204"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}