{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Binary for trianing a RNN-based classifier for the Quick, Draw! data.\\n\\npython train_model.py   --training_data train_data   --eval_data eval_data   --model_dir /tmp/quickdraw_model/   --cell_type cudnn_lstm\\n\\nWhen running on GPUs using --cell_type cudnn_lstm is much faster.\\n\\nThe expected performance is ~75% in 1.5M steps with the default configuration.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Binary for trianing a RNN-based classifier for the Quick, Draw! data.\n",
    "\n",
    "python train_model.py \\\n",
    "  --training_data train_data \\\n",
    "  --eval_data eval_data \\\n",
    "  --model_dir /tmp/quickdraw_model/ \\\n",
    "  --cell_type cudnn_lstm\n",
    "\n",
    "When running on GPUs using --cell_type cudnn_lstm is much faster.\n",
    "\n",
    "The expected performance is ~75% in 1.5M steps with the default configuration.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import ast\n",
    "import functools\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_classes():\n",
    "    classes = []\n",
    "    with tf.gfile.GFile(FLAGS.classes_file, \"r\") as f:\n",
    "        classes = [x for x in f]\n",
    "    num_classes = len(classes)\n",
    "    return num_classes\n",
    "\n",
    "\n",
    "def get_input_fn(mode, tfrecord_pattern, batch_size):\n",
    "    \"\"\"Creates an input_fn that stores all the data in memory.\n",
    "\n",
    "    Args:\n",
    "    mode: one of tf.contrib.learn.ModeKeys.{TRAIN, INFER, EVAL}\n",
    "    tfrecord_pattern: path to a TF record file created using create_dataset.py.\n",
    "    batch_size: the batch size to output.\n",
    "\n",
    "    Returns:\n",
    "        A valid input_fn for the model estimator.\n",
    "    \"\"\"\n",
    "\n",
    "    def _parse_tfexample_fn(example_proto, mode):\n",
    "        \"\"\"Parse a single record which is expected to be a tensorflow.Example.\"\"\"\n",
    "        feature_to_type = {\n",
    "            \"ink\": tf.VarLenFeature(dtype=tf.float32),\n",
    "            \"shape\": tf.FixedLenFeature([2], dtype=tf.int64)\n",
    "        }\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        # The labels won't be available at inference time, so don't add them\n",
    "        # to the list of feature_columns to be read.\n",
    "            feature_to_type[\"class_index\"] = tf.FixedLenFeature([1], dtype=tf.int64)\n",
    "\n",
    "        parsed_features = tf.parse_single_example(example_proto, feature_to_type)\n",
    "        labels = None\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            labels = parsed_features[\"class_index\"]\n",
    "        parsed_features[\"ink\"] = tf.sparse_tensor_to_dense(parsed_features[\"ink\"])\n",
    "        return parsed_features, labels\n",
    "\n",
    "    def _input_fn():\n",
    "        \"\"\"Estimator `input_fn`.\n",
    "\n",
    "        Returns:\n",
    "        A tuple of:\n",
    "        - Dictionary of string feature name to `Tensor`.\n",
    "        - `Tensor` of target labels.\n",
    "        \"\"\"\n",
    "        dataset = tf.data.TFRecordDataset.list_files(tfrecord_pattern)\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            dataset = dataset.shuffle(buffer_size=10)\n",
    "        dataset = dataset.repeat()\n",
    "        # Preprocesses 10 files concurrently and interleaves records from each file.\n",
    "        dataset = dataset.interleave(\n",
    "            tf.data.TFRecordDataset,\n",
    "            cycle_length=10,\n",
    "            block_length=1)\n",
    "        dataset = dataset.map(\n",
    "            functools.partial(_parse_tfexample_fn, mode=mode),\n",
    "            num_parallel_calls=10)\n",
    "        dataset = dataset.prefetch(10000)\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            dataset = dataset.shuffle(buffer_size=1000000)\n",
    "        # Our inputs are variable length, so pad them.\n",
    "        dataset = dataset.padded_batch(\n",
    "            batch_size, padded_shapes=dataset.output_shapes)\n",
    "        features, labels = dataset.make_one_shot_iterator().get_next()\n",
    "        return features, labels\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    \"\"\"Model function for RNN classifier.\n",
    "\n",
    "    This function sets up a neural network which applies convolutional layers (as\n",
    "    configured with params.num_conv and params.conv_len) to the input.\n",
    "    The output of the convolutional layers is given to LSTM layers (as configured\n",
    "    with params.num_layers and params.num_nodes).\n",
    "    The final state of the all LSTM layers are concatenated and fed to a fully\n",
    "    connected layer to obtain the final classification scores.\n",
    "\n",
    "    Args:\n",
    "        features: dictionary with keys: inks, lengths.\n",
    "        labels: one hot encoded classes\n",
    "        mode: one of tf.estimator.ModeKeys.{TRAIN, INFER, EVAL}\n",
    "        params: a parameter dictionary with the following keys: num_layers,\n",
    "        num_nodes, batch_size, num_conv, conv_len, num_classes, learning_rate.\n",
    "\n",
    "    Returns:\n",
    "        ModelFnOps for Estimator API.\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_input_tensors(features, labels):\n",
    "        \"\"\"Converts the input dict into inks, lengths, and labels tensors.\"\"\"\n",
    "        # features[ink] is a sparse tensor that is [8, batch_maxlen, 3]\n",
    "        # inks will be a dense tensor of [8, maxlen, 3]\n",
    "        # shapes is [batchsize, 2]\n",
    "        shapes = features[\"shape\"]\n",
    "        # lengths will be [batch_size]\n",
    "        lengths = tf.squeeze(tf.slice(shapes, begin=[0, 0], size=[params.batch_size, 1]))\n",
    "        inks = tf.reshape(features[\"ink\"], [params.batch_size, -1, 3])\n",
    "        if labels is not None:\n",
    "            labels = tf.squeeze(labels)\n",
    "        return inks, lengths, labels\n",
    "\n",
    "    def _add_conv_layers(inks, lengths):\n",
    "        \"\"\"Adds convolution layers.\"\"\"\n",
    "        convolved = inks\n",
    "        for i in range(len(params.num_conv)):\n",
    "            convolved_input = convolved\n",
    "            if params.batch_norm:\n",
    "                convolved_input = tf.layers.batch_normalization(\n",
    "                    convolved_input,\n",
    "                    training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "            # Add dropout layer if enabled and not first convolution layer.\n",
    "            if i > 0 and params.dropout:\n",
    "                convolved_input = tf.layers.dropout(\n",
    "                    convolved_input,\n",
    "                    rate=params.dropout,\n",
    "                    training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
    "            convolved = tf.layers.conv1d(\n",
    "                convolved_input,\n",
    "                filters=params.num_conv[i],\n",
    "                kernel_size=params.conv_len[i],\n",
    "                activation=None,\n",
    "                strides=1,\n",
    "                padding=\"same\",\n",
    "                name=\"conv1d_%d\" % i)\n",
    "        return convolved, lengths\n",
    "\n",
    "    def _add_regular_rnn_layers(convolved, lengths):\n",
    "        \"\"\"Adds RNN layers.\"\"\"\n",
    "        if params.cell_type == \"lstm\":\n",
    "            cell = tf.nn.rnn_cell.BasicLSTMCell\n",
    "        elif params.cell_type == \"block_lstm\":\n",
    "            cell = tf.contrib.rnn.LSTMBlockCell\n",
    "        cells_fw = [cell(params.num_nodes) for _ in range(params.num_layers)]\n",
    "        cells_bw = [cell(params.num_nodes) for _ in range(params.num_layers)]\n",
    "        if params.dropout > 0.0:\n",
    "            cells_fw = [tf.contrib.rnn.DropoutWrapper(cell) for cell in cells_fw]\n",
    "            cells_bw = [tf.contrib.rnn.DropoutWrapper(cell) for cell in cells_bw]\n",
    "        outputs, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "            cells_fw=cells_fw,\n",
    "            cells_bw=cells_bw,\n",
    "            inputs=convolved,\n",
    "            sequence_length=lengths,\n",
    "            dtype=tf.float32,\n",
    "            scope=\"rnn_classification\")\n",
    "        return outputs\n",
    "\n",
    "    def _add_cudnn_rnn_layers(convolved):\n",
    "        \"\"\"Adds CUDNN LSTM layers.\"\"\"\n",
    "        # Convolutions output [B, L, Ch], while CudnnLSTM is time-major.\n",
    "        convolved = tf.transpose(convolved, [1, 0, 2])\n",
    "        lstm = tf.contrib.cudnn_rnn.CudnnLSTM(\n",
    "            num_layers=params.num_layers,\n",
    "            num_units=params.num_nodes,\n",
    "            dropout=params.dropout if mode == tf.estimator.ModeKeys.TRAIN else 0.0,\n",
    "            direction=\"bidirectional\")\n",
    "        outputs, _ = lstm(convolved)\n",
    "        # Convert back from time-major outputs to batch-major outputs.\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        return outputs\n",
    "\n",
    "    def _add_rnn_layers(convolved, lengths):\n",
    "        \"\"\"Adds recurrent neural network layers depending on the cell type.\"\"\"\n",
    "        if params.cell_type != \"cudnn_lstm\":\n",
    "            outputs = _add_regular_rnn_layers(convolved, lengths)\n",
    "        else:\n",
    "            outputs = _add_cudnn_rnn_layers(convolved)\n",
    "        # outputs is [batch_size, L, N] where L is the maximal sequence length and N\n",
    "        # the number of nodes in the last layer.\n",
    "        mask = tf.tile(\n",
    "            tf.expand_dims(tf.sequence_mask(lengths, tf.shape(outputs)[1]), 2),\n",
    "            [1, 1, tf.shape(outputs)[2]])\n",
    "        zero_outside = tf.where(mask, outputs, tf.zeros_like(outputs))\n",
    "        outputs = tf.reduce_sum(zero_outside, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def _add_fc_layers(final_state):\n",
    "        \"\"\"Adds a fully connected layer.\"\"\"\n",
    "        return tf.layers.dense(final_state, params.num_classes)\n",
    "\n",
    "    # Build the model.\n",
    "    inks, lengths, labels = _get_input_tensors(features, labels)\n",
    "    convolved, lengths = _add_conv_layers(inks, lengths)\n",
    "    final_state = _add_rnn_layers(convolved, lengths)\n",
    "    logits = _add_fc_layers(final_state)\n",
    "    # Add the loss.\n",
    "    cross_entropy = tf.zx(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=logits))\n",
    "    # Add the optimizer.\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "        loss=cross_entropy,\n",
    "        global_step=tf.train.get_global_step(),\n",
    "        learning_rate=params.learning_rate,\n",
    "        optimizer=\"Adam\",\n",
    "        # some gradient clipping stabilizes training in the beginning.\n",
    "        clip_gradients=params.gradient_clipping_norm,\n",
    "        summaries=[\"learning_rate\", \"loss\", \"gradients\", \"gradient_norm\"])\n",
    "    # Compute current predictions.\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions={\"logits\": logits, \"predictions\": predictions},\n",
    "        loss=cross_entropy,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops={\"accuracy\": tf.metrics.accuracy(labels, predictions)})\n",
    "\n",
    "def create_estimator_and_specs(run_config):\n",
    "    \"\"\"Creates an Experiment configuration based on the estimator and input fn.\"\"\"\n",
    "    model_params = tf.contrib.training.HParams(\n",
    "        num_layers=FLAGS.num_layers,\n",
    "        num_nodes=FLAGS.num_nodes,\n",
    "        batch_size=FLAGS.batch_size,\n",
    "        num_conv=ast.literal_eval(FLAGS.num_conv),\n",
    "        conv_len=ast.literal_eval(FLAGS.conv_len),\n",
    "        num_classes=get_num_classes(),\n",
    "        learning_rate=FLAGS.learning_rate,\n",
    "        gradient_clipping_norm=FLAGS.gradient_clipping_norm,\n",
    "        cell_type=FLAGS.cell_type,\n",
    "        batch_norm=FLAGS.batch_norm,\n",
    "        dropout=FLAGS.dropout)\n",
    "\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        params=model_params)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=get_input_fn(\n",
    "        mode=tf.estimator.ModeKeys.TRAIN,\n",
    "        tfrecord_pattern=FLAGS.training_data,\n",
    "        batch_size=FLAGS.batch_size), max_steps=FLAGS.steps)\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=get_input_fn(\n",
    "        mode=tf.estimator.ModeKeys.EVAL,\n",
    "        tfrecord_pattern=FLAGS.eval_data,\n",
    "        batch_size=FLAGS.batch_size))\n",
    "\n",
    "    return estimator, train_spec, eval_spec\n",
    "\n",
    "def main(unused_args):\n",
    "    estimator, train_spec, eval_spec = create_estimator_and_specs(\n",
    "        run_config=tf.estimator.RunConfig(\n",
    "            model_dir=FLAGS.model_dir,\n",
    "            save_checkpoints_secs=300,\n",
    "            save_summary_steps=100))\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "model_dir should be non-empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 86\u001b[0m\n\u001b[1;32m     79\u001b[0m parser\u001b[39m.\u001b[39madd_argument(\n\u001b[1;32m     80\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m--self_test\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m     \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbool\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     82\u001b[0m     default\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFalse\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     83\u001b[0m     help\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWhether to enable batch normalization or not.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m FLAGS, unparsed \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse_known_args()\n\u001b[0;32m---> 86\u001b[0m tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mapp\u001b[39m.\u001b[39;49mrun(main\u001b[39m=\u001b[39;49mmain, argv\u001b[39m=\u001b[39;49m[sys\u001b[39m.\u001b[39;49margv[\u001b[39m0\u001b[39;49m]] \u001b[39m+\u001b[39;49m unparsed)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/tensorflow/python/platform/app.py:36\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Runs the program with an optional 'main' function and 'argv' list.\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m main \u001b[39m=\u001b[39m main \u001b[39mor\u001b[39;00m _sys\u001b[39m.\u001b[39mmodules[\u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmain\n\u001b[0;32m---> 36\u001b[0m _run(main\u001b[39m=\u001b[39;49mmain, argv\u001b[39m=\u001b[39;49margv, flags_parser\u001b[39m=\u001b[39;49m_parse_flags_tolerate_undef)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/absl/app.py:308\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    306\u001b[0m   callback()\n\u001b[1;32m    307\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m   _run_main(main, args)\n\u001b[1;32m    309\u001b[0m \u001b[39mexcept\u001b[39;00m UsageError \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    310\u001b[0m   usage(shorthelp\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, detailed_error\u001b[39m=\u001b[39merror, exitcode\u001b[39m=\u001b[39merror\u001b[39m.\u001b[39mexitcode)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/absl/app.py:254\u001b[0m, in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    252\u001b[0m   sys\u001b[39m.\u001b[39mexit(retval)\n\u001b[1;32m    253\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m   sys\u001b[39m.\u001b[39mexit(main(argv))\n",
      "Cell \u001b[0;32mIn[13], line 244\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(unused_args)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m(unused_args):\n\u001b[1;32m    243\u001b[0m     estimator, train_spec, eval_spec \u001b[39m=\u001b[39m create_estimator_and_specs(\n\u001b[0;32m--> 244\u001b[0m         run_config\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39;49mestimator\u001b[39m.\u001b[39;49mRunConfig(\n\u001b[1;32m    245\u001b[0m             model_dir\u001b[39m=\u001b[39;49mFLAGS\u001b[39m.\u001b[39;49mmodel_dir,\n\u001b[1;32m    246\u001b[0m             save_checkpoints_secs\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m,\n\u001b[1;32m    247\u001b[0m             save_summary_steps\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m))\n\u001b[1;32m    248\u001b[0m     tf\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39mtrain_and_evaluate(estimator, train_spec, eval_spec)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/tensorflow_estimator/python/estimator/run_config.py:551\u001b[0m, in \u001b[0;36mRunConfig.__init__\u001b[0;34m(self, model_dir, tf_random_seed, save_summary_steps, save_checkpoints_steps, save_checkpoints_secs, session_config, keep_checkpoint_max, keep_checkpoint_every_n_hours, log_step_count_steps, train_distribute, device_fn, protocol, eval_distribute, experimental_distribute, experimental_max_worker_delay_secs, session_creation_timeout_secs, checkpoint_save_graph_def)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[39mif\u001b[39;00m tf_config:\n\u001b[1;32m    549\u001b[0m   tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mTF_CONFIG environment variable: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, tf_config)\n\u001b[0;32m--> 551\u001b[0m model_dir \u001b[39m=\u001b[39m _get_model_dir(tf_config,\n\u001b[1;32m    552\u001b[0m                            compat_internal\u001b[39m.\u001b[39;49mpath_to_str(model_dir))\n\u001b[1;32m    554\u001b[0m RunConfig\u001b[39m.\u001b[39m_replace(\n\u001b[1;32m    555\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    556\u001b[0m     allowed_properties_list\u001b[39m=\u001b[39m_DEFAULT_REPLACEABLE_LIST,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    572\u001b[0m     session_creation_timeout_secs\u001b[39m=\u001b[39msession_creation_timeout_secs,\n\u001b[1;32m    573\u001b[0m     checkpoint_save_graph_def\u001b[39m=\u001b[39mcheckpoint_save_graph_def)\n\u001b[1;32m    575\u001b[0m \u001b[39m# TODO(frankchn,priyag): Eventually use distributed coordinator for TPUs.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/tensorflow_estimator/python/estimator/run_config.py:971\u001b[0m, in \u001b[0;36m_get_model_dir\u001b[0;34m(tf_config, model_dir)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[39m# pylint: disable=g-explicit-bool-comparison\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \n\u001b[1;32m    966\u001b[0m \u001b[39m# Empty string is treated as False in Python condition check, which triggers\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[39m# some confusing error messages. For example, 'a or b' returns None if a is ''\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[39m# and b is None. `None` is allowed for model_dir but '' is not allowed. Here,\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[39m# explicitly check empty string to provide clear error message.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[39mif\u001b[39;00m model_dir \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 971\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mmodel_dir should be non-empty.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    973\u001b[0m model_dir_in_tf_config \u001b[39m=\u001b[39m tf_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mmodel_dir\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m model_dir_in_tf_config \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: model_dir should be non-empty."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "    parser.add_argument(\n",
    "        \"--training_data\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Path to training data (tf.Example in TFRecord format)\")\n",
    "    parser.add_argument(\n",
    "        \"--eval_data\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Path to evaluation data (tf.Example in TFRecord format)\")\n",
    "    parser.add_argument(\n",
    "        \"--classes_file\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Path to a file with the classes - one class per line\")\n",
    "    parser.add_argument(\n",
    "        \"--num_layers\",\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help=\"Number of recurrent neural network layers.\")\n",
    "    parser.add_argument(\n",
    "        \"--num_nodes\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Number of node per recurrent network layer.\")\n",
    "    parser.add_argument(\n",
    "        \"--num_conv\",\n",
    "        type=str,\n",
    "        default=\"[48, 64, 96]\",\n",
    "        help=\"Number of conv layers along with number of filters per layer.\")\n",
    "    parser.add_argument(\n",
    "        \"--conv_len\",\n",
    "        type=str,\n",
    "        default=\"[5, 5, 3]\",\n",
    "        help=\"Length of the convolution filters.\")\n",
    "    parser.add_argument(\n",
    "        \"--cell_type\",\n",
    "        type=str,\n",
    "        default=\"lstm\",\n",
    "        help=\"Cell type used for rnn layers: cudnn_lstm, lstm or block_lstm.\")\n",
    "    parser.add_argument(\n",
    "        \"--batch_norm\",\n",
    "        type=\"bool\",\n",
    "        default=\"False\",\n",
    "        help=\"Whether to enable batch normalization or not.\")\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=0.0001,\n",
    "        help=\"Learning rate used for training.\")\n",
    "    parser.add_argument(\n",
    "        \"--gradient_clipping_norm\",\n",
    "        type=float,\n",
    "        default=9.0,\n",
    "        help=\"Gradient clipping norm used during training.\")\n",
    "    parser.add_argument(\n",
    "        \"--dropout\",\n",
    "        type=float,\n",
    "        default=0.3,\n",
    "        help=\"Dropout used for convolutions and bidi lstm layers.\")\n",
    "    parser.add_argument(\n",
    "        \"--steps\",\n",
    "        type=int,\n",
    "        default=100000,\n",
    "        help=\"Number of training steps.\")\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "        help=\"Batch size to use for training/evaluation.\")\n",
    "    parser.add_argument(\n",
    "        \"--model_dir\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Path for storing the model checkpoints.\")\n",
    "    parser.add_argument(\n",
    "        \"--self_test\",\n",
    "        type=\"bool\",\n",
    "        default=\"False\",\n",
    "        help=\"Whether to enable batch normalization or not.\")\n",
    "\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    tf.compat.v1.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ff18eac974d9cb58f7304f3b2e962230f58be49a1547696629541f9dd059ee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
