{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-c8134dad36314c49",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "WbMH-AjwyhTa"
      },
      "source": [
        "# CSE 204 Lab 13: Reinforcement Learning\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/adimajo/polytechnique-cse204-2019-releases/master/logo.jpg\" style=\"float: left; width: 15%\" />\n",
        "\n",
        "[CSE204-2019](https://moodle.polytechnique.fr/course/view.php?id=7862) Lab session #13\n",
        "\n",
        "Jérémie Decock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-239cad33053fa7d5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "L5FM1eD0yhTa"
      },
      "source": [
        "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adimajo/polytechnique-cse204-2019-releases/blob/master/lab_session_13/lab_session_13.ipynb)\n",
        "\n",
        "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/adimajo/polytechnique-cse204-2019-releases/master?filepath=lab_session_13%2Flab_session_13.ipynb)\n",
        "\n",
        "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/adimajo/polytechnique-cse204-2019-releases/raw/master/lab_session_13/lab_session_13.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDC24AGLyhTa"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The purpose of this lab is to introduce some classic concepts used\n",
        "in reinforcement learning like *Dynamic Programming*, *Bellman's Principle of Optimality*, *Bellman equations*, *value search* and *policy search*.\n",
        "$\n",
        "\\newcommand{\\vs}[1]{\\mathbf{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n",
        "\\newcommand{\\ms}[1]{\\mathbf{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n",
        "\\def\\U{V}\n",
        "\\def\\action{\\vs{a}}       % action\n",
        "\\def\\A{\\mathcal{A}}        % TODO\n",
        "\\def\\actionset{\\mathcal{A}} %%%\n",
        "\\def\\discount{\\gamma}  % discount factor\n",
        "\\def\\state{\\vs{s}}         % state\n",
        "\\def\\S{\\mathcal{S}}         % TODO\n",
        "\\def\\stateset{\\mathcal{S}}  %%%\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbgwVl1nyhTb"
      },
      "source": [
        "**Notice**: there are some differences in notations with the lecture slides, especially with the transition function: $T(\\state, \\action, \\state') \\equiv \\mathcal{P}(\\state' | \\state, \\action)$. Here we also assume that the reward only depends on the state: $r(\\state) \\equiv \\mathcal{R}(\\state, \\action, \\state')$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFGP_aPnyhTb"
      },
      "source": [
        "**Notice**: this notebook requires the OpenAI *Gym* library ; you can install it with `pip install gym` (the next cell does this for you if you use the Google Colab environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-49e97acfb570741d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "r0EXWpskyhTb"
      },
      "outputs": [],
      "source": [
        "colab_requirements = [\n",
        "    \"matplotlib>=3.1.2\",\n",
        "    \"numpy>=1.18.1\",\n",
        "    \"nose>=1.3.7\",\n",
        "    \"gym=>0.15.4\",\n",
        "]\n",
        "import sys, subprocess\n",
        "def run_subprocess_command(cmd):\n",
        "    # run the command\n",
        "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
        "    # print the output\n",
        "    for line in process.stdout:\n",
        "        print(line.decode().strip())\n",
        "        \n",
        "if \"google.colab\" in sys.modules:\n",
        "    for i in colab_requirements:\n",
        "        run_subprocess_command(\"pip install \" + i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlJOI9nsyhTb"
      },
      "source": [
        "You can uncomment the following cell to install gym in MyBinder or in your local environment (remove only the `#` not the `!`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91g1rrUFyhTb"
      },
      "outputs": [],
      "source": [
        "#!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o7SWXCOyhTb"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import math\n",
        "import gym\n",
        "import numpy as np\n",
        "import copy\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iF9a4-fyhTb"
      },
      "outputs": [],
      "source": [
        "sns.set_context(\"talk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAnFdlFcyhTb"
      },
      "outputs": [],
      "source": [
        "#matplotlib.rcParams['figure.figsize'] = (20.0, 10.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "toc-nb-collapsed": true,
        "id": "_fILUNEzyhTb"
      },
      "source": [
        "## Backward Induction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCb_Rz86yhTc"
      },
      "source": [
        "*Backward Induction* is a basic *Dynamic Programming* method **[BELLMAN57]**.\n",
        "Like other Dynamic Programming algorithms, it uses the *Bellman's\n",
        "Principle of Optimality* **[BELLMAN57]** for accelerating computation (compared\n",
        "to an exhaustive search). It can be applied to problems that exhibit a compatible structure, i.e., a problem that has *overlapping subproblems* or a problem having an *optimal substructure* **[BELLMAN57]**.\n",
        "Actually, this acceleration is obtained by breaking problems down into simpler subproblems in such a manner\n",
        "that redundant computations are avoided by storing results.\n",
        "When applicable, the method takes far less time than naïve methods that don't take advantage of the subproblem overlap (like depth-first search)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpigcVKQyhTc"
      },
      "source": [
        "*Backward Induction* computes non-stationary policies: a new policy is computed for each time step.\n",
        "Thus the number of time steps used to solve the problem is set in advance.\n",
        "*Backward Induction* algorithms solve Sequential Decision Making problems defined with\n",
        "discrete actions and state spaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiG3XHPPyhTc"
      },
      "source": [
        "The *value* (or *utility*) $\\U^*$ for each state $\\state$ at the latest time step $T$ is\n",
        "$$\n",
        "\\U^*_T(\\state) = r(\\state) \\label{eq:backward-induction-last-value} \\tag{1}\n",
        "$$\n",
        "where $r$ is the immediate reward function.\n",
        "\n",
        "The best expected value $\\U^*$ for each state $\\state$ at the $t^{\\text{th}}$ time step is\n",
        "$$\n",
        "\\U^*_t(\\state) = r(\\state) + \\max_{\\action \\in \\actionset} \\left[ \\sum_{\\state' \\in \\stateset} T(\\state, \\action, \\state') \\U^*_{t+1}(\\state') \\right]  \\label{eq:backward-induction-tth-value} \\tag{2}\n",
        "$$\n",
        "and the $t^{\\text{th}}$ optimal action (or decision) $d^*_t(\\state)$ among the set of\n",
        "possible actions $\\actionset$ is\n",
        "$$\n",
        "d^*_t(\\state) = \\arg\\max_{\\action \\in \\actionset} \\left[ \\sum_{\\state' \\in \\stateset} T(\\state, \\action, \\state') \\U^*_{t+1}(\\state') \\right]  \\label{eq:backward-induction-tth-decision} \\tag{3}\n",
        "$$\n",
        "where $T$ is the transition function.\n",
        "\n",
        "The main idea is to compute the expected value of each state\n",
        "(Eq. \\ref{eq:backward-induction-tth-value}) and then to use it to select the\n",
        "best action for any given state (Eq. \\ref{eq:backward-induction-tth-decision}).\n",
        "\n",
        "Eq. \\ref{eq:backward-induction-tth-value} cannot be solved analytically because\n",
        "the system of equations to compute $V$ contains non-linear terms (due to the\n",
        "\"max\" operator).\n",
        "As an alternative, Eq. \\ref{eq:backward-induction-tth-value}\n",
        "is usually computed using Dynamic Programming method, as described in algorithm 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqE30JsnyhTc"
      },
      "source": [
        "___\n",
        "### Algorithm 1: Backward Induction\n",
        "\n",
        "**Input**:<br>\n",
        "$\\quad$ $mdp = \\langle \\stateset, \\actionset, T, r \\rangle$, a Markov Decision Process <br>\n",
        "$\\quad$ $T$, the resolution horizon (i.e. the number of time steps) <br>\n",
        "**Local variables**: <br>\n",
        "$\\quad$ $\\U^*_t ~~ \\forall t \\in \\{1, ..., T\\}$, vectors of utilities for states in $\\stateset$ <br>\n",
        "<br>\n",
        "$\\U^*_T[\\state] \\leftarrow r(\\state) ~~ \\forall \\state \\in \\stateset$ <br>\n",
        "**for all** $t \\in \\{T-1, T-2, ..., 1\\}$ **do** <br>\n",
        "$\\quad$ **for all** $\\state \\in \\stateset$ **do** <br>\n",
        "$\\quad\\quad$ **if** $\\state$ is a final state **then** <br>\n",
        "$\\quad\\quad\\quad$ $\\displaystyle \\U^*_t[\\state] \\leftarrow r(\\state)$ <br>\n",
        "$\\quad\\quad$ **else** <br>\n",
        "$\\quad\\quad\\quad$ $\\displaystyle \\U^*_t[\\state] \\leftarrow r(\\state) + \\max_{\\action \\in \\actionset} \\left[ \\sum_{\\state' \\in \\stateset} T(\\state,\\action,\\state') \\U^*_{t+1}[\\state'] \\right]$ <br>\n",
        "$\\quad\\quad$ **end if** <br>\n",
        "$\\quad$ **end for** <br>\n",
        "**end for** <br>\n",
        "<br>\n",
        "**return** $\\U^*_t ~~ \\forall t \\in \\{1, ..., T\\}$\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "toc-nb-collapsed": true,
        "id": "XlUonoHmyhTc"
      },
      "source": [
        "## Value Iteration\n",
        "\n",
        "*Value Iteration* **[BELLMAN57]** is one of the most famous Dynamic Programming algorithm to compute the optimal policy for a Markov Decision Process (MDP).\n",
        "Similarly to Backward Induction, the\n",
        "main idea implemented by Value Iteration is to compute the best expected value of each state and then to use\n",
        "these values to select the best action from any given state.\n",
        "\n",
        "The main difference with the Backward Induction algorithm is that Value Iteration\n",
        "is used to compute stationary policies.\n",
        "Indeed, the same resulting policy is used for each time step and thus there is\n",
        "no assumption about the number of time steps to consider for the solution.\n",
        "\n",
        "The expected value $\\U^{\\pi}$ for each state $\\state$ when the agent follows a\n",
        "given (stationary) policy $\\pi$ is \n",
        "$$\n",
        "\\U^{\\pi}(\\state) = E \\left[ \\sum^{\\infty}_{t=0} \\discount^t r(\\state_t) | \\pi, \\state_0 = \\state \\right] \\label{eq:vi-value-of-s-for-pi} \\tag{4}\n",
        "$$\n",
        "\n",
        "The optimal (stationary) policy $\\pi^*$ is defined using the best expected value $\\U^{\\pi^*}$ and using the principle of *Maximum Expected Utility* as follows\n",
        "$$\n",
        "\\pi^*(\\state) = \\arg\\max_{\\action \\in \\actionset} \\left[ \\sum_{\\state' \\in \\stateset} T(\\state, \\action, \\state') \\U^{\\pi^*}(\\state') \\right]  \\label{eq:vi-optimal-policy} \\tag{5}\n",
        "$$\n",
        "\n",
        "Eq. \\ref{eq:vi-bellman-eq} is commonly called *Bellman equation*; it gives the best\n",
        "value we can expect for any given\n",
        "state (assuming the optimal policy $\\pi^*$ is\n",
        "followed). There are $|\\stateset|$ Bellman equations, one for each state.\n",
        "As for the Backward Induction method,\n",
        "this system of equations cannot be solved analytically because\n",
        "Bellman equations contain non-linear terms (due to the\n",
        "\"max\" operator).  As an alternative, Eq. \\ref{eq:vi-bellman-eq}\n",
        "can be computed iteratively using Value Iteration, a Dynamic Programming method\n",
        "described in Algorithm 2.\n",
        "\n",
        "\\begin{equation}\n",
        "    \\U(\\state) := \\U^{\\pi^*}(\\state) = \\left\\{\n",
        "    \\begin{array}{l l}\n",
        "        r(\\state)                                                                                                                                 & \\quad \\text{if $\\state$ is a final state} \\\\\n",
        "        \\displaystyle r(\\state) + \\discount \\max_{\\action \\in \\actionset} \\left[ \\sum_{\\state' \\in \\stateset} T(\\state, \\action, \\state') \\U(\\state') \\right]    & \\quad \\text{otherwise}\\\\\n",
        "    \\end{array} \\right.\n",
        "    \\label{eq:vi-bellman-eq} \\tag{6}\n",
        "\\end{equation}\n",
        "\n",
        "Equation \\ref{eq:vi-bellman-update} -- called *Bellman update* -- is\n",
        "used in the iterative method described in Algorithm 2, to update $\\U$ at each iteration.\n",
        "\n",
        "\\begin{equation}\n",
        "    \\U_{i+1}(\\state) \\leftarrow \\left\\{\n",
        "    \\begin{array}{l l}\n",
        "        r(\\state)                                                                                                                                   & \\quad \\text{if $\\state$ is a final state} \\\\\n",
        "        \\displaystyle r(\\state) + \\discount \\max_{\\action \\in \\actionset} \\left[ \\sum_{\\state' \\in \\stateset} T(\\state, \\action, \\state') \\U_i(\\state') \\right]    & \\quad \\text{otherwise}\\\\\n",
        "    \\end{array} \\right.\n",
        "    \\label{eq:vi-bellman-update} \\tag{7}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ajyYF5HyhTc"
      },
      "source": [
        "___\n",
        "### Algorithm 2: Value Iteration\n",
        "\n",
        "**Input**:<br>\n",
        "$\\quad$ $mdp = \\langle \\stateset, \\actionset, T, r \\rangle$, a Markov Decision Process <br>\n",
        "$\\quad$ $\\discount$, the discount factor <br>\n",
        "$\\quad$ $\\epsilon$, the maximum error allowed in the utility of any state in an iteration <br>\n",
        "**Local variables**: <br>\n",
        "$\\quad$ $\\U, \\U'$, old and new vectors of utilities for states in $\\stateset$, initially zero <br>\n",
        "$\\quad$ $\\delta$, the maximum change in the utility of any state in an iteration <br>\n",
        "<br>\n",
        "**repeat** <br>\n",
        "$\\quad$ $\\U \\leftarrow \\U'$ <br>\n",
        "$\\quad$ $\\delta \\leftarrow 0$ <br>\n",
        "$\\quad$ **for all** $\\state \\in \\stateset$ **do** <br>\n",
        "$\\quad\\quad$ **if** $\\state$ is a final state **then** <br>\n",
        "$\\quad\\quad\\quad$ $\\displaystyle \\U'[\\state] \\leftarrow r[\\state]$ <br>\n",
        "$\\quad\\quad$ **else** <br>\n",
        "$\\quad\\quad\\quad$ $\\displaystyle \\U'[\\state] \\leftarrow r[\\state] + \\discount \\max_{\\action \\in \\actionset} \\left[ \\sum_{\\state' \\in \\stateset} T(\\state,\\action,\\state') \\U[\\state'] \\right]$ <br>\n",
        "$\\quad\\quad$ **end if** <br>\n",
        "$\\quad\\quad$ **if** $|\\U'[\\state] - \\U[\\state]| > \\delta$ **then** <br>\n",
        "$\\quad\\quad\\quad$ $\\delta \\leftarrow |\\U'[\\state] - \\U[\\state]|$ <br>\n",
        "$\\quad\\quad$ **end if** <br>\n",
        "$\\quad$ **end for** <br>\n",
        "**until** $\\delta < \\epsilon(1-\\discount)/\\discount$ <br>\n",
        "<br>\n",
        "**return** $\\U$\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m95J3JjryhTc"
      },
      "source": [
        "### Convergence\n",
        "\n",
        "The convergence of Value Iteration has been proved, but this convergence is asymptotic **[BELLMAN57]**.\n",
        "However, each iteration is easy and fast to compute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "toc-nb-collapsed": true,
        "id": "a385AcvQyhTc"
      },
      "source": [
        "## Policy Iteration\n",
        "\n",
        "*Policy Iteration* **[HOWARD60]** is another popular Dynamic Programming algorithm to\n",
        "compute MDP's optimal policy. In practice, it is often faster than Value Iteration.\n",
        "\n",
        "The Policy Iteration algorithm alternates the following two steps, starting with an initial policy $\\pi_0$:\n",
        "1. Policy Evaluation: given a policy $\\pi_i$, compute $\\U^{\\pi_i}(\\state) ~ \\forall \\state \\in \\stateset$, the expected value of each state when $\\pi_i$ is followed.\n",
        "2. Policy Improvement: compute a new policy $\\pi_{i+1}$, using one-step look-ahead based on $\\U^{\\pi_i}$ and using the principle of *Maximum Expected Utility* as follows\n",
        "$$\n",
        "\\pi_{i+1}(\\state) = \\arg\\max_{\\action \\in \\actionset} \\left[ \\sum_{\\state' \\in \\stateset} T(\\state, \\action, \\state') \\U^{\\pi_i}(\\state') \\right]  \\label{eq:pi-policy-improvement} \\tag{8}\n",
        "$$\n",
        "\n",
        "Algorithm 3 describes the two-step procedure.\n",
        "The algorithm terminates when the *Policy Improvement* step yields no change in the utilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOQonDlUyhTc"
      },
      "source": [
        "___\n",
        "### Algorithm 3: Policy Iteration\n",
        "\n",
        "**Input**:<br>\n",
        "$\\quad$ $mdp = \\langle \\stateset, \\actionset, T, r \\rangle$, an MDP <br>\n",
        "**Local variables**: <br>\n",
        "$\\quad$ $\\U$, vector of utilities for states in $\\stateset$, initially zero <br>\n",
        "$\\quad$ $\\pi$, a policy vector indexed by state, initially random <br>\n",
        "<br>\n",
        "**repeat** <br>\n",
        "$\\quad$ $\\U \\leftarrow \\mbox{POLICY-EVALUATION}(\\pi, \\U, mdp)$ <br>\n",
        "$\\quad$ unchanged $\\leftarrow$ true <br>\n",
        "$\\quad$ **for all** $\\mbox{state} ~ \\state \\in \\stateset$ **do** <br>\n",
        "$\\quad\\quad$ **if** $\\displaystyle \\max_{\\action \\in \\actionset} \\left[ \\sum_{\\state' \\in \\stateset} T(\\state,\\action,\\state') \\U[\\state'] \\right] > \\sum_{\\state' \\in \\stateset} T(\\state,\\pi[\\state],\\state') \\U[\\state']$ **then** <br>\n",
        "$\\quad\\quad\\quad$ $\\displaystyle \\pi[\\state] \\leftarrow \\arg\\max_{\\action \\in \\actionset} \\left[ \\sum_{\\state' \\in \\stateset} T(\\state,\\action,\\state') \\U[\\state'] \\right]$ <br>\n",
        "$\\quad\\quad\\quad$ unchanged $\\leftarrow$ false <br>\n",
        "$\\quad\\quad$ **end if** <br>\n",
        "$\\quad$ **end for** <br>\n",
        "**until** unchanged <br>\n",
        "<br>\n",
        "**return** $\\pi$\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5bV2OC5yhTd"
      },
      "source": [
        "Solving the POLICY-EVALUATION routine is much simpler than solving the standard\n",
        "Bellman equations (which is what Value Iteration does).  Indeed, the action in each\n",
        "state is fixed by the policy, thus the \"max\" operator disappears and Bellman\n",
        "equations become linear.\n",
        "As a result, $\\U^{\\pi_i}$ can be computed by solving the linear system of these\n",
        "*simplified Bellman equations* (Eq. \\ref{eq:pi-simplified-bellman}) for\n",
        "each state.\n",
        "\n",
        "\\begin{equation}\n",
        "    \\U^{\\pi_i}(\\state) = \\left\\{\n",
        "    \\begin{array}{l l}\n",
        "        r(\\state)                                                                                                          & \\quad \\text{if $\\state$ is a final state} \\\\\n",
        "        \\displaystyle r(\\state) + \\discount \\sum_{\\state' \\in \\stateset} T(\\state, \\pi_i(\\state), \\state') \\U^{\\pi_i}(\\state')    & \\quad \\text{otherwise}\\\\\n",
        "    \\end{array} \\right.\n",
        "    \\label{eq:pi-simplified-bellman} \\tag{9}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ9MM6zlyhTd"
      },
      "source": [
        "### Convergence\n",
        "\n",
        "As the number of states and policies is finite, and as the policy is improved\n",
        "at each iteration, Policy Iteration converges in a finite number of iterations (often\n",
        "small in practice).\n",
        "However, within each iteration, solving the\n",
        "POLICY-EVALUATION routine may cost a lot (its complexity is $O(|\\stateset|^3)$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc-hr-collapsed": true,
        "toc-nb-collapsed": true,
        "id": "Dok_2RuQyhTd"
      },
      "source": [
        "## Hands on OpenAI Gym and the FrozenLake toy problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy7pyEaiyhTd"
      },
      "source": [
        "For the purpose of focusing on the algorithms, we will use standard environments provided by OpenAI Gym framework.\n",
        "OpenAI Gym provides controllable environments (https://gym.openai.com/envs/) for research in Reinforcement Learning.\n",
        "We will use a simple toy problem to illustrate Dynamic Programming algorithms properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdUK7F4xyhTd"
      },
      "source": [
        "**Task:** read https://gym.openai.com/docs/ to discover Gym and get familiar with its main concepts.\n",
        "\n",
        "In this lab, we will try to solve the FrozenLake-v0 environment (https://gym.openai.com/envs/FrozenLake-v0/).\n",
        "Additional information is available [here](https://github.com/openai/gym/wiki/FrozenLake-v0) and [here](https://github.com/openai/gym/wiki/FrozenLake-v0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fcuuL0AyhTd"
      },
      "source": [
        "**Notice**: this environment is *fully observable*, thus here the terms (environment) *state* and (agent) *observation* are equivalent.\n",
        "This is not always the case for example in poker, the agent doesn't know the opponent's cards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJmu96rUyhTd"
      },
      "source": [
        "### Get the FrozenLake state space and action space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVLIQUuMyhTd"
      },
      "outputs": [],
      "source": [
        "env = gym.make('FrozenLake-v0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5xu87WPyhTd"
      },
      "source": [
        "Possible states in FrozenLake are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3uCgimxyhTd"
      },
      "outputs": [],
      "source": [
        "states = list(range(env.observation_space.n))\n",
        "states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ9X8FAsyhTd"
      },
      "source": [
        "Possible actions are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_9lBRB5yhTd"
      },
      "outputs": [],
      "source": [
        "actions = list(range(env.action_space.n))\n",
        "actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-hADDrTyhTd"
      },
      "source": [
        "The following dictionary may be used to understand actions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrPuH92ayhTd"
      },
      "outputs": [],
      "source": [
        "action_labels = {\n",
        "    0: \"Move Left\",\n",
        "    1: \"Move Down\",\n",
        "    2: \"Move Right\",\n",
        "    3: \"Move Up\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jx1nANSyhTd"
      },
      "source": [
        "### Display functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weZR0jxkyhTd"
      },
      "source": [
        "The next cells contain functions that can be used to display states, transitions and policies with the FrozenLake environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbiEEJ2hyhTd"
      },
      "outputs": [],
      "source": [
        "def states_display(state_seq, title=None, figsize=(5,5), annot=True, fmt=\"0.1f\", linewidths=.5, square=True, cbar=False, cmap=\"Reds\"):\n",
        "    size = int(math.sqrt(len(state_seq)))\n",
        "    state_array = np.array(state_seq)\n",
        "    state_array = state_array.reshape(size, size)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=figsize)         # Sample figsize in inches\n",
        "    sns.heatmap(state_array, annot=annot, fmt=fmt, linewidths=linewidths, square=square, cbar=cbar, cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4dDcIrqyhTd"
      },
      "outputs": [],
      "source": [
        "def transition_display(state, action):\n",
        "    states_display(transition_array[state,action], title=\"Transition probabilities for action {} ({}) in state {}\".format(action, action_labels[action], state))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv5rmJ-QyhTd"
      },
      "outputs": [],
      "source": [
        "def display_policy(policy):\n",
        "    actions_src = [\"{}={}\".format(action, action_labels[action].replace(\"Move \", \"\")) for action in actions]\n",
        "    title = \"Policy (\" + \", \".join(actions_src) + \")\"\n",
        "    states_display(policy, title=title, fmt=\"d\", cbar=False, cmap=\"Reds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsC9nIyNyhTd"
      },
      "source": [
        "### Make the `is_final_array`, `reward_array` and `transition_array`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfYhaFaHyhTd"
      },
      "source": [
        "To implement Dynamic Programming algorithms, we need the transition probability (or transition function) and the reward function, both defined in `env.P`.\n",
        "\n",
        "`env.P[S][A]` gives the list of reachable states from state S executing action A.\n",
        "\n",
        "These reachable states are coded in a tuple defined like this: `(probability, next state, reward, is_final_state)`.\n",
        "\n",
        "You will not need to use `env.P` to solve exercises.\n",
        "In the following cell, `is_final_array`, `reward_array` and `transition_array` are defined for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRY26qtlyhTd"
      },
      "outputs": [],
      "source": [
        "is_final_array = np.full(shape=len(states), fill_value=np.nan, dtype=np.bool)\n",
        "reward_array = np.full(shape=len(states), fill_value=np.NINF)                # np.NINF = negative infinity\n",
        "transition_array = np.zeros(shape=(len(states), len(actions), len(states)))\n",
        "\n",
        "for state in states:\n",
        "    for action in actions:\n",
        "        for next_state_tuple in env.P[state][action]:              # env.P[state][action] contains the next states list (a list of tuples)\n",
        "            transition_probability, next_state, next_state_reward, next_state_is_final = next_state_tuple\n",
        "\n",
        "            is_final_array[next_state] = next_state_is_final\n",
        "            reward_array[next_state] = max(reward_array[next_state], next_state_reward)   # workaround: when we already are in state 15, reward is 0 if we stay in state 15 (in practice this never append as the simulation stop when we arrive in state 15 as any other terminal state)\n",
        "            transition_array[state, action, next_state] += transition_probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGpQ3BoOyhTd"
      },
      "outputs": [],
      "source": [
        "def reachable_states(state, action):\n",
        "    return np.nonzero(transition_array[state, action])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUqJMk_FyhTe"
      },
      "source": [
        "The following plot shows the state corresponding to square of the FrozenLake grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtaqlawXyhTe"
      },
      "outputs": [],
      "source": [
        "states_display(states, fmt=\"d\", title=\"States ID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTFQPal6yhTe"
      },
      "source": [
        "The following plot shows the reward obtained in each square of the FrozenLake grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_exlZimvyhTe"
      },
      "outputs": [],
      "source": [
        "states_display(reward_array, title=\"Rewards\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbssDXOEyhTe"
      },
      "source": [
        "The following plot shows whether a square is a final state or not (i.e. whether it ends the simulation or not)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Geuat1BByhTe"
      },
      "outputs": [],
      "source": [
        "states_display(is_final_array, fmt=\"d\", title=\"Final states\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZTeUos8yhTe"
      },
      "source": [
        "The following cells show how to display transitions with the provided `transition_display` function. Figures displayed in squares are the probability to reach these squares from the given (`state`, `action`) pair. Colored squares are the states that may be reached from this pair (a non-zero probability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B79flnheyhTe"
      },
      "outputs": [],
      "source": [
        "transition_display(state=0, action=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZTgURBeyhTe"
      },
      "outputs": [],
      "source": [
        "transition_display(state=6, action=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7wWKVWUyhTe"
      },
      "outputs": [],
      "source": [
        "transition_display(state=6, action=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FKZX1bCyhTe"
      },
      "source": [
        "## Exercise 1: Implement the Value Iteration algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjY3Zd-SyhTe"
      },
      "source": [
        "To solve the FrozenLake-v0 problem with Dynamic Programming, we will first use the Value Iteration algorithm described in Algorithm 2.\n",
        "\n",
        "Notice that the FrozenLake-v0 environment is non-deterministic.\n",
        "To implement Value Iteration, you will need the transition probability (or the transition function) defined in `transition_array`.\n",
        "- Use `reachable_states(S, A)` to get the list of reachable states from state `S` executing action `A`.\n",
        "- Use `transition_array[S, A]` to get the probability of reaching each state from state `S` executing action `A`.\n",
        "- Use `transition_array[S, A, S']` to get the probability of reaching state `S'` from state `S` executing action `A`.\n",
        "\n",
        "You will also need the previously defined `is_final_array` matrix.\n",
        "- Use `is_final_array[S]` to know whether `S` is a final state (`True`) or not (`False`).\n",
        "\n",
        "Finally, you will need the previously defined `reward_array` matrix.\n",
        "- Use `reward_array[S]` to get the reward obtained by the agent each time it reaches state `S`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvMWuQOUyhTe"
      },
      "source": [
        "In the following cell, we define `expected_value` and `expected_values` functions for convenience.\n",
        "The first one returns the expected reward\n",
        "$$\\sum T(\\state, \\action, \\state') \\U(\\state')$$\n",
        "for a given pair $(\\state, \\action)$ and a given V-table (value function) $\\U$.\n",
        "The second one computes the expected reward for all the actions in $\\state$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FE-5ZQdyhTe"
      },
      "outputs": [],
      "source": [
        "def expected_value(state, action, v_array):\n",
        "    return (transition_array[state, action] * v_array).sum() # compute sum(T(s,a,s').V(s'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20QHM6aDyhTe"
      },
      "outputs": [],
      "source": [
        "def expected_values(state, v_array):\n",
        "    return (transition_array[state] * v_array).sum(axis=1)   # compute sum(T(s,a,s').V(s')) for all the actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ80rpDkyhTe"
      },
      "source": [
        "### Question 1: Implement the Value Iteration algorithm (compute the *value function* `v_array`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg3RvMP4yhTe"
      },
      "source": [
        "**Note**: here we use the `state_display` function to show the evolution of the value function `v_array` over iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vnXvq2QyhTe"
      },
      "outputs": [],
      "source": [
        "stop = False\n",
        "\n",
        "value_function_history = []\n",
        "delta_history = []\n",
        "\n",
        "def value_iteration(gamma=0.95, epsilon=0.001, display=False):\n",
        "    v_array = np.zeros(len(states))   # Initial value function\n",
        "    stop = False\n",
        "\n",
        "    while not stop:\n",
        "        if display:\n",
        "            states_display(v_array, title=\"Value function\", cbar=True, cmap=\"Reds\")\n",
        "        else:\n",
        "            print('.', end=\"\")\n",
        "        value_function_history.append(v_array)\n",
        "        \n",
        "        delta = 0.\n",
        "        \n",
        "        # TODO...\n",
        "        \n",
        "        delta_history.append(delta)\n",
        "        \n",
        "        if delta < epsilon:\n",
        "            stop = True\n",
        "    \n",
        "    return v_array\n",
        "        \n",
        "v_array = value_iteration(display=True)\n",
        "states_display(v_array, title=\"Value function\", cbar=True, cmap=\"Reds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TqJb8gsyhTe"
      },
      "source": [
        "### Display the evolution of the value function over iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q622FxE_yhTe"
      },
      "outputs": [],
      "source": [
        "df_v_hist = pd.DataFrame(value_function_history)\n",
        "df_v_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_pxEh6CyhTe"
      },
      "source": [
        "Evolution of `v_array` (the estimated value of each state) over iterations (one curve per state):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y8Wry4FyhTe"
      },
      "outputs": [],
      "source": [
        "df_v_hist.plot()\n",
        "plt.title(\"V(s) w.r.t iteration\")\n",
        "plt.ylabel(\"V(s)\")\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.legend(loc='upper right');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1OIVwY2yhTe"
      },
      "source": [
        "Evolution of `delta` over iterations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pns91heOyhTe"
      },
      "outputs": [],
      "source": [
        "plt.plot(delta_history)\n",
        "plt.yscale(\"log\")\n",
        "plt.title(r\"$\\max~\\delta$ w.r.t iteration\")\n",
        "plt.ylabel(r\"$\\max~\\delta$\")\n",
        "plt.xlabel(\"iteration\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J53EExLyyhTe"
      },
      "source": [
        "### Question 2: Define the greedy policy (Maximum Expected Utility)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-SSgrtryhTf"
      },
      "outputs": [],
      "source": [
        "def greedy_policy(state, v_array):\n",
        "    \n",
        "    # TODO...\n",
        "    \n",
        "    return policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STKbvaIxyhTf"
      },
      "source": [
        "### Display the opimized policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAFTZFWUyhTf"
      },
      "source": [
        "Applying the `greedy_policy` on each state gives us the policy matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5NlOExkyhTf"
      },
      "outputs": [],
      "source": [
        "policy = [greedy_policy(state, v_array) for state in states]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rizb6bwNyhTg"
      },
      "source": [
        "The following cell gives us a graphical representation of the optimal policy we have computed. The figure in each square is the optimal action to execute in the corresponding state (0 = \"move left\", 1 = \"move down\", 2 = \"move right\", 3 = \"move up\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyzIujDWyhTg"
      },
      "outputs": [],
      "source": [
        "display_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS2teDSvyhTg"
      },
      "source": [
        "### Evaluate Value Iteration with Gym (single trial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onCFX4OLyhTh"
      },
      "source": [
        "So far, we have computed the value function `v_array` for one *episode*.\n",
        "The environment is stochastic, thus if we apply the computed policy several times on the environment, we may have different results.\n",
        "To measure the performance of our value function `v_array`, we should assess it several times and count the number of successful trials.\n",
        "OpenAI considers an agent to successfully solve the FrozenLake problem if it reaches 76% success rate over the last 100 trials (or \"episodes\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYcGWVpHyhTh"
      },
      "outputs": [],
      "source": [
        "env._max_episode_steps = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRa4XwFVyhTh"
      },
      "outputs": [],
      "source": [
        "reward_list = []\n",
        "\n",
        "NUM_EPISODES = 1000\n",
        "\n",
        "for episode_index in range(NUM_EPISODES):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    #t = 0\n",
        "\n",
        "    while not done:\n",
        "        action = greedy_policy(state, v_array)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        #t += 1\n",
        "\n",
        "    reward_list.append(reward)\n",
        "    #print(\"Episode finished after {} timesteps ; reward = {}\".format(t, reward))\n",
        "\n",
        "print(sum(reward_list) / NUM_EPISODES)            \n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ9fYvnbyhTh"
      },
      "source": [
        "### Question 3: What do you think the discount factor $\\gamma$ is for?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oki486uyhTh"
      },
      "source": [
        "TODO..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYVm6QkoyhTh"
      },
      "source": [
        "### Evaluate Value Iteration for different value of $\\gamma$ with confidence interval (bootstrap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjymUAYmyhTh"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "NUM_EPISODES = 1000\n",
        "\n",
        "reward_list = []\n",
        "\n",
        "for gamma in (0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.84, 0.9, 0.95, 0.99):\n",
        "    v_array = value_iteration(gamma=gamma)\n",
        "    \n",
        "    for episode_index in range(NUM_EPISODES):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = greedy_policy(state, v_array)\n",
        "            state, reward, done, info = env.step(action)\n",
        "\n",
        "        reward_list.append({\"gamma\": gamma, \"reward\": reward})\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hZmqyGVyhTh"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(reward_list)\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G_HCTMpyhTh"
      },
      "outputs": [],
      "source": [
        "# Plot mean reward (with its 95% confidence interval)\n",
        "\n",
        "sns.relplot(x=\"gamma\", y=\"reward\", kind=\"line\", data=df, height=6, aspect=1.5)\n",
        "plt.axhline(0.76, color=\"red\", linestyle=\":\", label=\"76% success threshold\");   # 76% success threshold\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylyre2XtyhTh"
      },
      "source": [
        "### Display the Value Iteration optimal policy with respect to $\\gamma$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cjLza5VyhTh"
      },
      "outputs": [],
      "source": [
        "for gamma in (0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.84, 0.9, 0.95, 0.99):\n",
        "    print()\n",
        "    print(\"=\" * 10, \"GAMMA = \", gamma, \"=\" * 10)\n",
        "    print()\n",
        "    \n",
        "    v_array = value_iteration(gamma=gamma)\n",
        "    \n",
        "    print()\n",
        "    print()\n",
        "    \n",
        "    policy = [greedy_policy(state, v_array) for state in states]\n",
        "    display_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFeekCN3yhTh"
      },
      "source": [
        "## Exercise 2: Implement the Policy Iteration algorithm (Bonus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CgRBTYTyhTh"
      },
      "source": [
        "An approach alternative to Value Iteration (Exercise 1) is Policy Iteration (described in Algorithm 3).\n",
        "\n",
        "**Task:** implement Iterative Policy Iteration (for the same environment). Note that as part of this task you should also implement iterative policy evaluation. Compare the policies obtained by both approaches (they should be the same)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CCHSPYMyhTh"
      },
      "source": [
        "### Question 1: Define the (exact) Policy Evaluation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1DiVx_3yhTh"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(policy, gamma):\n",
        "    \n",
        "    # TODO...\n",
        "    \n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kMFi37kyhTh"
      },
      "source": [
        "### Question 2: Define the Policy Improvement function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XXfV1kIyhTh"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(gamma, initial_policy=None, policy_evaluation_function=policy_evaluation):\n",
        "    \n",
        "    # TODO...\n",
        "\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCT-qOiayhTh"
      },
      "outputs": [],
      "source": [
        "gamma = 0.99\n",
        "\n",
        "policy = policy_iteration(gamma=gamma, policy_evaluation_function=policy_evaluation)\n",
        "\n",
        "display_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNrp1QeeyhTh"
      },
      "source": [
        "### Evaluate Policy Iteration with Gym (single trial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1xl2tyLyhTh"
      },
      "outputs": [],
      "source": [
        "env._max_episode_steps = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BpYst-KyhTh"
      },
      "outputs": [],
      "source": [
        "reward_list = []\n",
        "\n",
        "NUM_EPISODES = 1000\n",
        "\n",
        "for episode_index in range(NUM_EPISODES):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    #t = 0\n",
        "\n",
        "    while not done:\n",
        "        action = policy[state]      # Take a random action\n",
        "        state, reward, done, info = env.step(action)\n",
        "        #t += 1\n",
        "\n",
        "    reward_list.append(reward)\n",
        "    #print(\"Episode finished after {} timesteps ; reward = {}\".format(t, reward))\n",
        "\n",
        "print(sum(reward_list) / NUM_EPISODES)            \n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3ll51cfyhTh"
      },
      "source": [
        "### Evaluate Policy Iteration for different $\\gamma$ with confidence interval (bootstrap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-3w6zEcyhTh"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "NUM_EPISODES = 1000\n",
        "\n",
        "reward_list = []\n",
        "\n",
        "for gamma in (0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.84, 0.9, 0.95, 0.99):\n",
        "    print(\"gamma:\", gamma)\n",
        "    policy = policy_iteration(gamma=gamma)\n",
        "    \n",
        "    for episode_index in range(NUM_EPISODES):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            state, reward, done, info = env.step(action)\n",
        "\n",
        "        reward_list.append({\"gamma\": gamma, \"reward\": reward})\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK6ZRDOeyhTi"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(reward_list)\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAEaDsn7yhTi"
      },
      "source": [
        "### Plot mean reward (with its 95% confidence interval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDjz-YelyhTi"
      },
      "outputs": [],
      "source": [
        "sns.relplot(x=\"gamma\", y=\"reward\", kind=\"line\", data=df, height=6, aspect=1.5)\n",
        "plt.axhline(0.76, color=\"red\", linestyle=\":\", label=\"76% success threshold\");   # 76% success threshold\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-b82aa116ac616c8d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "6V0TDatxyhTi"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuGpl2yDyhTi"
      },
      "source": [
        "**[BELLMAN57]** Richard Ernest Bellman. *Dynamic Programming*. Princeton University Press, Princeton,\n",
        "New Jersey, USA, 1957."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBlFYuj4yhTi"
      },
      "source": [
        "**[HOWARD60]** R.A. Howard. Dynamic Programming and Markov Processes. MIT Press, Cambridge,\n",
        "Massachusetts, 1960."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-b82aa116ac616c8d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "8GgkeiIhyhTi"
      },
      "source": [
        "## Going further\n",
        "\n",
        "In this lab we have introduced Reinforcement Learning in a very specific case where the *agent* (the algorithm) has a perfect knowledge of the environment (transition and reward functions).\n",
        "\n",
        "This is convenient to introduce basic concepts but we cannot expect this assumption to be true in many practical problems.\n",
        "A lot of sophisticated algorithms have been developed recently and most of them have been implemented in [OpenAI Baselines](https://openai.com/blog/openai-baselines-dqn/) library and can be used in [OpenAI Gym](https://gym.openai.com/) benchmark library.\n",
        "\n",
        "Also, for those who want to go further, one of the best book in reinforcement learning is freely available on the web: http://incompleteideas.net/book/RLbook2018.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S9f1zDtyhTi"
      },
      "source": [
        "Example of what can be done in RL:\n",
        "- AlphaGo (movie) https://www.youtube.com/watch?v=WXuK6gekU1Y (this work had huge impact in the AI community)\n",
        "- AlphaGo https://deepmind.com/research/case-studies/alphago-the-story-so-far\n",
        "- AlphaZero https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go\n",
        "- AlphaStar (StarCraft II) https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning\n",
        "- DQN https://deepmind.com/blog/article/deep-reinforcement-learning\n",
        "- Dota 2 https://openai.com/blog/openai-five/\n",
        "- Robotics https://openai.com/blog/solving-rubiks-cube/\n",
        "- Robotics https://openai.com/blog/learning-dexterity/\n",
        "- Breakout https://www.youtube.com/watch?v=V1eYniJ0Rnk\n",
        "- Walker https://youtu.be/pgaEE27nsQw\n",
        "- Helicopter https://www.youtube.com/watch?v=VCdxqn0fcnE\n",
        "- Energy https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40\n",
        "- Self-driving cars"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}